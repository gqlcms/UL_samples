{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os,subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import datetime\n",
    "# in order to show the plots\n",
    "%matplotlib inline\n",
    "\n",
    "crab_word=\"\"\n",
    "crab_user = \"x509up_u123238\"# put here your crab user (the one that gets returned after voms etc etc)\n",
    "subprocess.getstatusoutput(\"echo \"+crab_word+\" | voms-proxy-init -voms cms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search all the UL samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.call('dasgoclient --query=\"dataset dataset=/*/*Summer20UL*/NANOAODSIM\" > ULSamples.txt', shell=True)\n",
    "# ProcessedUL = open(\"ULSamples.txt\",'r').readlines()\n",
    "subprocess.call('dasgoclient --query=\"dataset dataset=/*/*Summer20UL18NanoAODv2*/NANOAODSIM\" > ULSamples18.txt', shell=True)\n",
    "ProcessedUL18 = open(\"ULSamples18.txt\",'r').readlines()\n",
    "subprocess.call('dasgoclient --query=\"dataset dataset=/*/*Summer20UL17NanoAODv2*/NANOAODSIM\" > ULSamples17.txt', shell=True)\n",
    "ProcessedUL17 = open(\"ULSamples17.txt\",'r').readlines()\n",
    "subprocess.call('dasgoclient --query=\"dataset dataset=/*/*Summer20UL16NanoAO*v2*/NANOAODSIM\" > ULSamples16.txt', shell=True)\n",
    "ProcessedUL16 = open(\"ULSamples16.txt\",'r').readlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "Search UL datasets\n",
    "1. search by key words\n",
    "    1. key words is a dic, like:<br>\n",
    "        \"VVV\":{\"elect\":\\[(\"WWW\"),(\"ZZZ\",\"TuneCP5\")\\],\"exclude\"\\[(\"HZ\", \"HHH\")\\]}<br>\n",
    "        means if (\"WWW\") or (\"ZZZ\",\"TuneCP5\") in the datasets, will select this dataset<br>\n",
    "        (\"ZZZ\",\"TuneCP5\") means \"ZZZ\" and \"TuneCP5\" should both in the datasets name<br>\n",
    "        (\"HZ\", \"HHH\") means if \"HZ\" and \"HHH\" both in the datasets name, this dataset will be excluded<br>\n",
    "    2. Get_FullInformation_ForDataSets:\n",
    "        1. return information we need for All datasets\n",
    "            1. for one datasets\n",
    "                1. 'info' keeps the list got from dasgoclient : \\[{\"file_size\":147036563,\"nblocks\":1,\"nevents\":77000,\"nfiles\":1,\"nlumis\":77,\"num_block\":1,\"num_event\":77000,\"num_file\":1,\"num_lumi\":77}\\]\n",
    "                2. 'group' keeps the group information : 'VVV'\n",
    "                3. DatasetName : '/ZZZ_TuneCP5_13TeV-amcatnlo-pythia8/RunIISummer20UL17NanoAODv2-106X_mc2017_realistic_v8-v1/NANOAODSIM'\n",
    "2. store the information into table\n",
    "    1. one group will be stored in one lattice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UL samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0lepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = \"16\" ; \n",
    "# year = \"17\" ; \n",
    "year = \"18\" ; \n",
    "channel = \"0Lepton\"\n",
    "ProcessedUL = eval(\"ProcessedUL\"+year)\n",
    "\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/samples.py\".format( channel = channel , year = year )\n",
    "Text_File = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )\n",
    "Table_pngName = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.png\".format( channel = channel , year = year )\n",
    "\n",
    "TextFile_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file_v7 = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}.txt\".format( channel = channel , year = year )\n",
    "missed_text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Lepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"16\" ; \n",
    "# year = \"17\" ; \n",
    "# year = \"18\" ; \n",
    "channel = \"1Lepton\"\n",
    "ProcessedUL = eval(\"ProcessedUL\"+year)\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/samples.py\".format( channel = channel , year = year )\n",
    "Text_File = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )\n",
    "Table_pngName = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.png\".format( channel = channel , year = year )\n",
    "\n",
    "TextFile_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file_v7 = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}.txt\".format( channel = channel , year = year )\n",
    "missed_text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Lepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = \"16\" ; \n",
    "year = \"17\" ; \n",
    "# year = \"18\" ; \n",
    "channel = \"2Lepton_OS\"\n",
    "ProcessedUL = eval(\"ProcessedUL\"+year)\n",
    "\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/samples.py\".format( channel = channel , year = year )\n",
    "Text_File = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )\n",
    "Table_pngName = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.png\".format( channel = channel , year = year )\n",
    "\n",
    "TextFile_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file_v7 = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}.txt\".format( channel = channel , year = year )\n",
    "missed_text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4Lepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = \"16\" ; \n",
    "# year = \"17\" ; \n",
    "year = \"18\" ; \n",
    "channel = \"4Lepton\"\n",
    "ProcessedUL = eval(\"ProcessedUL\"+year)\n",
    "\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/samples.py\".format( channel = channel , year = year )\n",
    "Text_File = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )\n",
    "Table_pngName = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.png\".format( channel = channel , year = year )\n",
    "\n",
    "TextFile_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}\".format( channel = channel , year = year )\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "Text_file_v7 = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}.txt\".format( channel = channel , year = year )\n",
    "missed_text_file = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/NanoAOD_v7_20{year}_missied.py\".format( channel = channel , year = year )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output for v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NanoAOD_v7_for_Comparison_ = NanoAOD_v7_for_Comparison( Text_file = Text_file_v7)\n",
    "sample_dic = NanoAOD_v7_for_Comparison_.TextFile_To_dic()\n",
    "missing_dic = NanoAOD_v7_for_Comparison_.Generate_TextFile_To_Pickup_Missed_File(sample_dic = sample_dic, missed_text_file = missed_text_file)\n",
    "table_group = NanoAOD_v7_for_Comparison_.InfoForTable( sample_dic = sample_dic)\n",
    "NanoAOD_v7_for_Comparison_.Mutiple_GroupTabel( table_group = table_group, colLabels = [\"dataset nanoAOD v7\"],  Table_dir = base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyWords = {}\n",
    "\n",
    "exec(open(base_path+\"Keywords.py\",\"r\").read())\n",
    "# PartOf_Group = ['1Lepton_18_EGamma','1Lepton_18_QCD_HT', '1Lepton_18_SingleMuon', '1Lepton_18_ST', '1Lepton_18_TT', \"1Lepton_18_VV\", ]\n",
    "# KeyWords = dict([(key,KeyWords[key]) for key in PartOf_Group])\n",
    "# if the tuple in the text only have one element and no comma in the end, the tuple will be read as string, not tuple\n",
    "for group in KeyWords:\n",
    "    for elect_exclude in KeyWords[group]:\n",
    "        KeyWords_group_elect_exclude = KeyWords[group][elect_exclude] ; KeyWords[group][elect_exclude] = []\n",
    "        for i in KeyWords_group_elect_exclude:\n",
    "            if type(i) != type((1,2)):\n",
    "                KeyWords[group][elect_exclude].append(tuple([i]))\n",
    "            else:\n",
    "                KeyWords[group][elect_exclude].append(i)\n",
    "print(KeyWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UL datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Search_UL_datasets_ = Search_UL_datasets(ULDatasets = ProcessedUL, KeyWords = KeyWords)\n",
    "UL_datasets_Weneed = Search_UL_datasets_.Search_dataset_By_KeyWords()\n",
    "Search_UL_datasets_.Generate_Sample_py(Text_file = Text_file, UL_datasets_Weneed = UL_datasets_Weneed)\n",
    "UL_datasets_Weneed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UL tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summury = Search_UL_datasets_.Get_FullInformation_ForDataSets()\n",
    "table = Search_UL_datasets_.InfoForTable()\n",
    "Table_dir = base_path\n",
    "Search_UL_datasets_.Mutiple_GroupTabel( Table_dir = Table_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pring_Missed_ULSamples_ = Pring_Missed_ULSamples(Text_File = missed_text_file)\n",
    "table_group = Pring_Missed_ULSamples_.Table_info( )\n",
    "Pring_Missed_ULSamples_.Mutiple_GroupTabel( table_group = table_group, colLabels = [\"missed sample\"],  Table_pngName = Table_pngName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search UL samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search_UL_datasets():\n",
    "    \n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.ULDatasets = kwargs.get(\"ULDatasets\",[])\n",
    "        self.KeyWords = kwargs.get(\"KeyWords\",[])\n",
    "        self.tablecolumns = kwargs.get(\"tablecolumns\",[\"UL dataset\", \"Nevents\",'raw_height'])\n",
    "        self.tablecolumns_for_plot = kwargs.get(\"tablecolumns_for_plot\",['UL dataset','Nevents'])\n",
    "        self.Table_dir = \"Table/\"\n",
    "        \n",
    "    def Search_dataset_By_KeyWords(self, **kwargs):\n",
    "        KeyWords = kwargs.get(\"KeyWords\",self.KeyWords)\n",
    "        ULDatasets = kwargs.get(\"ULDatasets\",self.ULDatasets)\n",
    "        \n",
    "        UL_datasets_Weneed = dict([(group,[]) for group in KeyWords])\n",
    "        for dataset in ULDatasets:\n",
    "            for group in KeyWords:\n",
    "                elect_Words = KeyWords[group].get(\"elect\",[])\n",
    "                exclude_Words = KeyWords[group].get(\"exclude\",[])\n",
    "                # ielect : the element of the elect_Words and exclude_Words are list, ielect contains tuple\n",
    "                # if all the words in the tuple is true, ielect is true\n",
    "                # if one of the ielect is true, then elect the dataset\n",
    "                if len(elect_Words) > 0:\n",
    "                    Exclude_this_Dataset_Final = False\n",
    "                    for iexclude in exclude_Words:\n",
    "                        if len(iexclude) > 0 :\n",
    "                            Exclude_this_Dataset = True\n",
    "                            for iexclude_single in iexclude:\n",
    "                                if iexclude_single not in dataset:\n",
    "                                    Exclude_this_Dataset = False\n",
    "                        if Exclude_this_Dataset : Exclude_this_Dataset_Final = True\n",
    "                    if Exclude_this_Dataset_Final: continue\n",
    "                    for ielect in elect_Words :\n",
    "                        if (len(ielect) > 0) & (type(ielect) == type((1,2))):\n",
    "                            ielect_this_Dataset = True\n",
    "                            for ielect_single_words in ielect:\n",
    "                                if ielect_single_words not in dataset:\n",
    "                                    ielect_this_Dataset = False\n",
    "                            if ielect_this_Dataset:\n",
    "                                if (dataset.rstrip(\"\\n\")) not in UL_datasets_Weneed[group]:\n",
    "                                    UL_datasets_Weneed[group].append(dataset.rstrip(\"\\n\"))\n",
    "        self.UL_datasets_Weneed = UL_datasets_Weneed\n",
    "        return UL_datasets_Weneed\n",
    "                            \n",
    "        \n",
    "    def Get_FullInformation_ForDataSets(self, **kwargs):\n",
    "        UL_datasets_Weneed = kwargs.get(\"UL_datasets_Weneed\",self.UL_datasets_Weneed)\n",
    "        \n",
    "        appendText1='dasgoclient -query=\\\"summary dataset='\n",
    "        datasets_List_Weneed = [] ; summury = {}\n",
    "        \n",
    "        for group in UL_datasets_Weneed:\n",
    "            summury[group] = []\n",
    "            for i in UL_datasets_Weneed[group]:\n",
    "                print(i)\n",
    "                info = subprocess.getstatusoutput(appendText1+i+'\"')[1]\n",
    "                if 'file_size' in info:\n",
    "                    Info = {}; \n",
    "                    Info['info'] = eval(info) ; \n",
    "                    Info['group'] = group\n",
    "                    Info['DatasetName'] = i\n",
    "                    summury[group].append(Info)\n",
    "        self.summury = summury\n",
    "        return summury\n",
    "        \n",
    "        \n",
    "    def InfoForTable(self, **kwargs):\n",
    "        summury = kwargs.get(\"summury\",self.summury)\n",
    "        tablecolumns = kwargs.get(\"tablecolumns\",self.tablecolumns)\n",
    "        \n",
    "        table_group = {}\n",
    "        for group in summury:\n",
    "            table_group[group] = dict([(columns,[]) for columns in tablecolumns])\n",
    "            first = True ; notsave = True; group_height_in_table = len(summury[group])\n",
    "            dataset_name_str = ''\n",
    "            Nevents_str = ''\n",
    "            group_str = ''\n",
    "            if len(summury[group])>0:\n",
    "                \n",
    "                for datasetinfo in summury[group]:\n",
    "                    dataset_name =  datasetinfo['DatasetName']\n",
    "                    Nevents = datasetinfo['info'][0]['nevents']\n",
    "                    group = datasetinfo['group']\n",
    "                    dataset_name_str += dataset_name + '\\n'\n",
    "                    Nevents_str += str(Nevents)+'\\n'\n",
    "                    group_str += ''+'\\n'\n",
    "            table_group[group]['UL dataset'].append(dataset_name_str)\n",
    "            table_group[group]['Nevents'].append(Nevents_str)\n",
    "            table_group[group]['raw_height'].append(group_height_in_table)\n",
    "        self.table = table_group\n",
    "        return table_group\n",
    "    \n",
    "    def Mutiple_GroupTabel(self, **kwargs):\n",
    "        Info_Table = kwargs.get(\"Info_Table\",self.table)\n",
    "        Table_dir = kwargs.get(\"Table_dir\",self.Table_dir)\n",
    "        table_group = kwargs.get(\"table_group\",self.table)\n",
    "        for group in table_group:\n",
    "            Table_pngName = \"{Table_dir}/{group}/{Table_name}.png\".format(Table_dir = Table_dir, group = group, Table_name = group)\n",
    "            Table_pngPath = \"{Table_dir}/{group}/\".format(Table_dir = Table_dir, group = group, Table_name = group)\n",
    "            if not os.path.isfile(Table_pngPath):\n",
    "                print(\"mkdir -p \"+Table_pngPath)\n",
    "                os.system(\"mkdir -p \"+Table_pngPath)\n",
    "            self.Generate_table(Info_Table = self.table[group] , sample_name = Table_pngName)\n",
    "        \n",
    "    \n",
    "    def Generate_table(self, **kwargs):\n",
    "        Info_Table = kwargs.get(\"Info_Table\",)\n",
    "        table_size = kwargs.get(\"table_size\",(10,10))\n",
    "        colLabels = kwargs.get(\"colLabels\",self.tablecolumns_for_plot)\n",
    "        colWidths = kwargs.get(\"colWidths\",[0.9,0.1])\n",
    "        sample_name = kwargs.get(\"sample_name\",\"test.png\")\n",
    "        \n",
    "        table_height = 0.1\n",
    "    #     table_height = 0.3\n",
    "        for i in Info_Table['raw_height']:\n",
    "            table_height+=i*0.05\n",
    "        table_height = table_height*2.5\n",
    "        table_size = (10,table_height)\n",
    "        fig,ax = plt.subplots(figsize = table_size , dpi=200)\n",
    "        ax.set_title(\"\",fontsize = 24)\n",
    "        ax.xaxis.set_ticks([]) # remove x axis\n",
    "        ax.yaxis.set_ticks([])\n",
    "        df  =pd.DataFrame(Info_Table)[colLabels]\n",
    "        table = ax.table(cellText = df.values,\n",
    "                  cellLoc='left',\n",
    "                  cellColours = None,\n",
    "                  colColours=['#F3CC32', '#2769BD', '#DC3735'],\n",
    "    #               colLabels = df.columns,\n",
    "                  colLabels = colLabels,\n",
    "    #               height=height,\n",
    "                  colWidths = colWidths,\n",
    "                  rowLoc='bottom',\n",
    "                  loc='bottom',\n",
    "                  bbox = [0,0,1,1],\n",
    "                 )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(5)\n",
    "        cellDict = table.get_celld()\n",
    "        for i in range(0,len(df.columns)):\n",
    "            cellDict[(0,i)].set_height(.1)\n",
    "            for j in range(1,len(Info_Table['raw_height'])+1):\n",
    "                cellDict[(j,i)].set_height(Info_Table['raw_height'][j-1]*0.05+0.05)\n",
    "#         plt.show()\n",
    "        plt.savefig(sample_name)\n",
    "    \n",
    "    def Generate_Sample_py(self, **kwargs):\n",
    "        Text_file = kwargs.get(\"Text_file\", \"\")\n",
    "        UL_datasets_Weneed = kwargs.get(\"UL_datasets_Weneed\", )\n",
    "        Templete = kwargs.get(\"Templete\", '        DBSSample(dataset=\"{dataset}\")               : \"\",\\n' )\n",
    "        \n",
    "        out_str = \"data = {\\n\"\n",
    "        for group in UL_datasets_Weneed:\n",
    "            for ds in UL_datasets_Weneed[group]:\n",
    "                out_str += Templete.format(dataset = ds)\n",
    "        out_str += \"}\\n\"\n",
    "        \n",
    "        with open(Text_file, \"w\") as f:\n",
    "                f.write(out_str)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pring Missed ULSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pring_Missed_ULSamples():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.Text_File = kwargs.get(\"Text_File\",\"\")\n",
    "            \n",
    "    def Table_info(self, **kwargs):\n",
    "        Text_File = kwargs.get(\"Text_File\",  self.Text_File)\n",
    "        have_missing_file = False\n",
    "        \n",
    "        f = open(Text_File,\"r\")\n",
    "        sample_dic_old = eval(f.read())\n",
    "        sample_dic = {}\n",
    "        for key in sample_dic_old:\n",
    "            ds_list = []\n",
    "            for j in sample_dic_old[key]:\n",
    "                if  \"miss\" in sample_dic_old[key][j] :\n",
    "                    have_missing_file = True\n",
    "                    ds_list.append(j)\n",
    "            if len(ds_list)>0:\n",
    "                sample_dic[key] = ds_list\n",
    "        \n",
    "        table_group = {}\n",
    "        for group in sample_dic:\n",
    "            table_group[group] = {\"dataset nanoAOD v7\":[],\"raw_height\":[]}\n",
    "            group_height_in_table = 2;\n",
    "            dataset_name_str = ''\n",
    "            for dataset in sample_dic[group]:\n",
    "                dataset = dataset.replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "                if dataset:\n",
    "                    dataset_name_str += dataset + '\\n'\n",
    "                    group_height_in_table += 1\n",
    "            table_group[group]['dataset nanoAOD v7'].append(dataset_name_str)\n",
    "            table_group[group]['raw_height'].append(group_height_in_table)\n",
    "        \n",
    "        return table_group\n",
    "    \n",
    "    def Mutiple_GroupTabel(self, **kwargs):\n",
    "        Table_pngName = kwargs.get(\"Table_pngName\", \"\")\n",
    "        table_group = kwargs.get(\"table_group\", \"\")\n",
    "        colLabels = kwargs.get(\"colLabels\", \"\")\n",
    "        \n",
    "\n",
    "        table_new = {}\n",
    "        ds_new = [] ; height_new = [] ;\n",
    "        for group in table_group:\n",
    "            ds_new.append(table_group[group]['dataset nanoAOD v7'][0])\n",
    "            height_new.append(table_group[group]['raw_height'][0])\n",
    "        table_new['missed sample'] = ds_new\n",
    "        table_new['raw_height'] = height_new\n",
    "            \n",
    "        self.Generate_table(Info_Table = table_new , sample_name = Table_pngName, colLabels = colLabels)\n",
    "            \n",
    "    def Generate_table(self, **kwargs):\n",
    "        Info_Table = kwargs.get(\"Info_Table\",)\n",
    "        table_size = kwargs.get(\"table_size\",(10,10))\n",
    "        colLabels = kwargs.get(\"colLabels\", [])\n",
    "        colWidths = kwargs.get(\"colWidths\",[1])\n",
    "        sample_name = kwargs.get(\"sample_name\",\"test.png\")\n",
    "        \n",
    "        table_height = 0.1\n",
    "    #     table_height = 0.3\n",
    "        for i in Info_Table['raw_height']:\n",
    "            table_height+=i*0.05\n",
    "        table_height = table_height*2.5\n",
    "        table_size = (10,table_height)\n",
    "        fig,ax = plt.subplots(figsize = table_size , dpi=200)\n",
    "        ax.set_title(\"\",fontsize = 24)\n",
    "        ax.xaxis.set_ticks([]) # remove x axis\n",
    "        ax.yaxis.set_ticks([])\n",
    "        df  =pd.DataFrame(Info_Table)[colLabels]\n",
    "        table = ax.table(cellText = df.values,\n",
    "                  cellLoc='left',\n",
    "                  cellColours = None,\n",
    "                  colColours=['#F3CC32', '#2769BD', '#DC3735'],\n",
    "    #               colLabels = df.columns,\n",
    "                  colLabels = colLabels,\n",
    "    #               height=height,\n",
    "                  colWidths = colWidths,\n",
    "                  rowLoc='bottom',\n",
    "                  loc='bottom',\n",
    "                  bbox = [0,0,1,1],\n",
    "                 )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(5)\n",
    "        cellDict = table.get_celld()\n",
    "        for i in range(0,len(df.columns)):\n",
    "            cellDict[(0,i)].set_height(.1)\n",
    "            for j in range(1,len(Info_Table['raw_height'])+1):\n",
    "                cellDict[(j,i)].set_height(Info_Table['raw_height'][j-1]*0.05+0.05)\n",
    "#         plt.show()\n",
    "        plt.savefig(sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put NanoAOD v7 sample for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     105
    ]
   },
   "outputs": [],
   "source": [
    "def remove_space(a):\n",
    "    return a.replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "\n",
    "class NanoAOD_v7_for_Comparison():\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.NanoAOD_v7_info = {\n",
    "            \"Text_file\" : kwargs.get(\"Text_file\",\"\"),\n",
    "        }\n",
    "    \n",
    "    def TextFile_To_dic(self, **kwargs):\n",
    "        Text_file = kwargs.get(\"Text_file\", self.NanoAOD_v7_info[\"Text_file\"])\n",
    "        \n",
    "        f = open(Text_file,\"r\")\n",
    "        dic_str = \"\"\n",
    "        for i in f.readlines():\n",
    "            if len(i.replace(\" \",\"\").replace(\"\\n\",\"\")) > 0:\n",
    "                dic_str += i\n",
    "        sample_dic_old = eval(dic_str)\n",
    "        sample_dic = {}\n",
    "        for key in sample_dic_old:\n",
    "            sample_dic[key.replace(\" \",\"\")] = sample_dic_old[key]\n",
    "        f.close()\n",
    "        return sample_dic\n",
    "    \n",
    "    def InfoForTable(self, **kwargs):\n",
    "        sample_dic = kwargs.get(\"sample_dic\",  {})\n",
    "        \n",
    "        table_group = {}\n",
    "        for group in sample_dic:\n",
    "            table_group[group] = {\"dataset nanoAOD v7\":[],\"raw_height\":[]}\n",
    "            group_height_in_table = 2;\n",
    "            dataset_name_str = ''\n",
    "            for dataset in sample_dic[group].split(\"\\n\"):\n",
    "                dataset = dataset.replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "                if dataset:\n",
    "                    dataset_name_str += dataset + '\\n'\n",
    "                    group_height_in_table += 1\n",
    "            table_group[group]['dataset nanoAOD v7'].append(dataset_name_str)\n",
    "            table_group[group]['raw_height'].append(group_height_in_table)\n",
    "        self.table = table_group\n",
    "        return table_group\n",
    "    \n",
    "    def Mutiple_GroupTabel(self, **kwargs):\n",
    "        Table_dir = kwargs.get(\"Table_dir\", \"\")\n",
    "        table_group = kwargs.get(\"table_group\", \"\")\n",
    "        colLabels = kwargs.get(\"colLabels\", \"\")\n",
    "        \n",
    "        for group in table_group:\n",
    "            Table_pngName = \"{Table_dir}/{group}/NanoAOD_v7{Table_name}.png\".format(Table_dir = Table_dir, group = group, Table_name = group)\n",
    "            Table_pngPath = \"{Table_dir}/{group}/\".format(Table_dir = Table_dir, group = group, Table_name = group)\n",
    "            if not os.path.isfile(Table_pngPath):\n",
    "                print(\"mkdir -p \"+Table_pngPath)\n",
    "                os.system(\"mkdir -p \"+Table_pngPath)\n",
    "            self.Generate_table(Info_Table = table_group[group] , sample_name = Table_pngName, colLabels = colLabels)\n",
    "            \n",
    "    def Generate_table(self, **kwargs):\n",
    "        Info_Table = kwargs.get(\"Info_Table\",)\n",
    "        table_size = kwargs.get(\"table_size\",(10,10))\n",
    "        colLabels = kwargs.get(\"colLabels\", [])\n",
    "        colWidths = kwargs.get(\"colWidths\",[1])\n",
    "        sample_name = kwargs.get(\"sample_name\",\"test.png\")\n",
    "        \n",
    "        table_height = 0.1\n",
    "    #     table_height = 0.3\n",
    "        for i in Info_Table['raw_height']:\n",
    "            table_height+=i*0.05\n",
    "        table_height = table_height*2.5\n",
    "        table_size = (10,table_height)\n",
    "        fig,ax = plt.subplots(figsize = table_size , dpi=200)\n",
    "        ax.set_title(\"\",fontsize = 24)\n",
    "        ax.xaxis.set_ticks([]) # remove x axis\n",
    "        ax.yaxis.set_ticks([])\n",
    "        df  =pd.DataFrame(Info_Table)[colLabels]\n",
    "        table = ax.table(cellText = df.values,\n",
    "                  cellLoc='left',\n",
    "                  cellColours = None,\n",
    "                  colColours=['#F3CC32', '#2769BD', '#DC3735'],\n",
    "    #               colLabels = df.columns,\n",
    "                  colLabels = colLabels,\n",
    "    #               height=height,\n",
    "                  colWidths = colWidths,\n",
    "                  rowLoc='bottom',\n",
    "                  loc='bottom',\n",
    "                  bbox = [0,0,1,1],\n",
    "                 )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(5)\n",
    "        cellDict = table.get_celld()\n",
    "        for i in range(0,len(df.columns)):\n",
    "            cellDict[(0,i)].set_height(.1)\n",
    "            for j in range(1,len(Info_Table['raw_height'])+1):\n",
    "                cellDict[(j,i)].set_height(Info_Table['raw_height'][j-1]*0.05+0.05)\n",
    "#         plt.show()\n",
    "        plt.savefig(sample_name)\n",
    "    \n",
    "    def Generate_TextFile_To_Pickup_Missed_File(self, **kwargs):\n",
    "        sample_dic = kwargs.get(\"sample_dic\",  {})\n",
    "        missed_text_file = kwargs.get(\"missed_text_file\",  \"\")\n",
    "        \n",
    "        missed_dic = {}\n",
    "        for group in sample_dic:\n",
    "            missed_dic[group] = {}\n",
    "            for ds in (sample_dic[group]).split(\"\\n\"):\n",
    "                ds = remove_space(ds)\n",
    "                if ds:\n",
    "                    missed_dic[group][ds] = \"\"\n",
    "                    \n",
    "        missed_dic_text = \"{\\n\"\n",
    "        for i in missed_dic:\n",
    "            missed_dic_text += '    \"'+i+'\":{\\n'\n",
    "            for j in missed_dic[i]:\n",
    "                missed_dic_text += '         \"'+j+'\":\"\",\\n'\n",
    "            missed_dic_text += '    },\\n'\n",
    "        missed_dic_text += \"}\\n\"\n",
    "                    \n",
    "        with open(missed_text_file,\"w\") as f:\n",
    "            f.write(str(missed_dic_text))\n",
    "                    \n",
    "        return missed_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample.py to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sample_To_TextFile(sample, debug = False):\n",
    "    if debug:\n",
    "        print('sample file :', sample)\n",
    "    for i in open(sample,\"r\").readlines():\n",
    "        if \"DBSSample\" in i:\n",
    "            remove_Comment_out = re.compile(r'^((\\s*)#)')\n",
    "            catch_dataset = re.compile(r'dataset=\"(.*)\"\\)')\n",
    "            remove = remove_Comment_out.search(i)\n",
    "            if debug:\n",
    "                if remove:\n",
    "                    print(\"remove :\",i)\n",
    "            if not remove:\n",
    "                print( catch_dataset.search(i).groups()[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0lepnton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyWords = {}\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/0Lepton/18/\"\n",
    "exec(open(base_path+\"Keywords.py\",\"r\").read())\n",
    "List_1Lepton18 = [i for i in list(KeyWords.keys()) if \"0Lepton_18\" in i]\n",
    "print('{')\n",
    "for i in List_1Lepton18:\n",
    "    print('\"',i,'\": \"\"\"\\n\\n\"\"\",','\\n\\n')\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1lepnton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyWords = {}\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/1Lepton/18/\"\n",
    "exec(open(base_path+\"Keywords.py\",\"r\").read())\n",
    "List_1Lepton18 = [i for i in list(KeyWords.keys()) if \"1Lepton_18\" in i]\n",
    "print('{')\n",
    "for i in List_1Lepton18:\n",
    "    print('\"',i,'\": \"\"\"\\n\\n\"\"\",','\\n\\n')\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2lepton_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"18\" ; channel = \"2Lepton_OS\"\n",
    "\n",
    "KeyWords = {}\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/{channel}/{year}/\".format( channel = channel , year = year )\n",
    "exec(open(base_path+\"Keywords.py\",\"r\").read())\n",
    "List_ = [i for i in list(KeyWords.keys()) ]\n",
    "print('{')\n",
    "for i in List_:\n",
    "    print('\"',i,'\": \"\"\"\\n\\n\"\"\",','\\n\\n')\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample_To_TextFile(\"/eos/user/q/qiguo/www/VVV/UL_samples/2Lepton_OS/18/sample.txt\", debug = True)\n",
    "Sample_To_TextFile(\"/eos/user/q/qiguo/www/VVV/UL_samples/2Lepton_OS/16/sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4lepnton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyWords = {}\n",
    "base_path = \"/eos/user/q/qiguo/www/VVV/UL_samples/4Lepton/18/\"\n",
    "exec(open(base_path+\"Keywords.py\",\"r\").read())\n",
    "List_1Lepton18 = [i for i in list(KeyWords.keys()) if \"4Lepton_18\" in i]\n",
    "print('{')\n",
    "for i in List_1Lepton18:\n",
    "    print('\"',i,'\": \"\"\"\\n\\n\"\"\",','\\n\\n')\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_To_TextFile(\"/eos/user/q/qiguo/www/VVV/UL_samples/4Lepton/16/sample.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
